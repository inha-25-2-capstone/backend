# CLAUDE.md

This file provides guidance to Claude Code when working with code in this repository.

## Project Overview

**Political News Aggregation and Analysis Backend** - A hybrid cloud architecture system that scrapes Korean political news from Naver News, performs AI-powered summarization and stance analysis (ÏòπÌò∏/Ï§ëÎ¶Ω/ÎπÑÌåê), clusters articles by topic, and provides multi-perspective recommendations.

The system uses **Render** for web application hosting and a **separate AI service** (deployed on Hugging Face Spaces or similar) for AI processing, connected via asynchronous task queues for efficient batch operations.

## Team Structure

- **Backend Developer**: FastAPI, Celery, PostgreSQL, Redis, scraper, deployment
- **Frontend Developer**: React application (separate repository)
- **ML Engineer**: Fine-tuning stance analysis model in Colab

## Technology Stack

### Render Platform (Web Application Hosting)
- **Web Service**: FastAPI backend API server
- **PostgreSQL**: Database with pgvector extension (Singapore region)
- **Background Worker**: Celery workers for batch processing
- **Cron Jobs**: Scheduled tasks for scraping and clustering
- **Redis**: Task queue for async job management

### AI Service (Separate Repository/Deployment)
- **Deployment**: Hugging Face Spaces (or alternative)
- **FastAPI Server**: Independent AI microservice
- **Batch Summarization API**: Article summarization endpoint
- **Batch Stance Analysis API**: Topic-based stance detection endpoint (using fine-tuned model)

### Development Stack
- **Python 3.12** with virtual environment
- **Web Scraping**: Selenium 4.35.0 (headless Chrome) + BeautifulSoup4
- **Task Queue**: Celery + Redis
- **API Framework**: FastAPI 0.119.0
- **Database**: PostgreSQL 16 + pgvector extension
- **Database Migrations**: Alembic 1.13.2
- **Configuration**: python-dotenv for environment management
- **Docker**: Docker Compose for local development

## Project Structure

```
backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/                      # FastAPI application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py               # Main FastAPI app
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/               # API endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ topics.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ articles.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas/              # Pydantic models
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ responses.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/                 # Web scraping
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraper.py            # Naver News scraper
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ workers/                  # Celery tasks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ celery_app.py         # Celery configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks.py              # Celery tasks
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py             # Worker settings
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/                 # Business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clustering.py         # Topic clustering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recommendation.py     # Recommendation engine
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ai_client.py          # AI service API client
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                   # Database models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py           # DB connection & ORM
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                    # Utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py             # Logging configuration
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ config.py                 # Environment configuration
‚îÇ
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ postgre_schema.sql        # Database schema (reference)
‚îÇ   ‚îî‚îÄ‚îÄ migrations/               # Alembic migrations
‚îÇ       ‚îú‚îÄ‚îÄ env.py                # Migration environment config
‚îÇ       ‚îú‚îÄ‚îÄ script.py.mako        # Migration template
‚îÇ       ‚îî‚îÄ‚îÄ versions/             # Migration version files
‚îÇ
‚îú‚îÄ‚îÄ scripts/                      # Executable scripts
‚îÇ   ‚îú‚îÄ‚îÄ run_scraper.py            # Scraper entry point
‚îÇ   ‚îú‚îÄ‚îÄ run_clustering.py         # Clustering entry point
‚îÇ   ‚îú‚îÄ‚îÄ init_db.py                # Database initialization
‚îÇ   ‚îî‚îÄ‚îÄ migrate.py                # Migration helper script
‚îÇ
‚îú‚îÄ‚îÄ data/                         # Local test data (git ignored)
‚îú‚îÄ‚îÄ logs/                         # Log files (git ignored)
‚îú‚îÄ‚îÄ tests/                        # Test suite
‚îÇ
‚îú‚îÄ‚îÄ .env                          # Local environment variables (git ignored)
‚îú‚îÄ‚îÄ .env.example                  # Environment variable template
‚îú‚îÄ‚îÄ .gitignore                    # Git ignore rules
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ alembic.ini                   # Alembic configuration
‚îú‚îÄ‚îÄ Dockerfile                    # Production Docker image
‚îú‚îÄ‚îÄ docker-compose.yml            # Local development environment
‚îú‚îÄ‚îÄ render.yaml                   # Render deployment configuration
‚îú‚îÄ‚îÄ CLAUDE.md                     # This file
‚îî‚îÄ‚îÄ README.md                     # Project documentation
```

## Development Setup

### Prerequisites
- Python 3.12
- Docker and Docker Compose
- Git

### Initial Setup

```bash
# 1. Clone repository
cd /home/zedwrkc/backend

# 2. Create virtual environment
python3.12 -m venv venv
source venv/bin/activate  # Linux/Mac

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure environment variables
cp .env.example .env
# Edit .env with your local settings
# IMPORTANT: Add this line to .env for local development:
echo 'DATABASE_URL=postgresql://postgres:postgres@localhost:5432/politics_news_dev' >> .env

# 5. Start Docker services (PostgreSQL + Redis)
docker compose up -d  # Note: 'docker compose' not 'docker-compose'

# 6. Initialize database with Alembic migrations
python scripts/init_db.py

# 7. Verify setup
docker compose ps  # Check services are running
python scripts/migrate.py current  # Check migration status
```

### Important Notes for Local Development

1. **Environment Variables:**
   - If you have system-level `DATABASE_URL` set (e.g., from Render), unset it for local development:
     ```bash
     unset DATABASE_URL
     ```
   - Or ensure `.env` file has the local DATABASE_URL which will override system env

2. **Running Scraper:**
   ```bash
   # Make sure to unset DATABASE_URL if it points to production
   unset DATABASE_URL
   source venv/bin/activate
   python scripts/run_scraper.py
   ```

3. **Docker Commands:**
   - Use `docker compose` (without hyphen) for modern Docker installations
   - If `docker-compose` command not found, use `docker compose` instead

### Development Workflow

```bash
# Terminal 1: Start FastAPI backend
source venv/bin/activate
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000

# Terminal 2: Start Celery worker
source venv/bin/activate
celery -A src.workers.celery_app worker --loglevel=info

# Terminal 3: Run scraper manually (for testing)
source venv/bin/activate
python scripts/run_scraper.py

# Terminal 4: Run clustering manually (for testing)
source venv/bin/activate
python scripts/run_clustering.py
```

### Docker Commands

```bash
# Start services
docker-compose up -d

# Stop services
docker-compose down

# View logs
docker-compose logs -f postgres
docker-compose logs -f redis

# Reset database (WARNING: destroys all data)
docker-compose down -v
docker-compose up -d
```

## Database Architecture

The schema (`database/postgre_schema.sql`) implements a normalized 6-table design:

### Core Tables

1. **press** - News organizations
2. **article** - Full article content with metadata
   - `news_date`: Calculated daily news cycle (KST 5:00 cutoff)
3. **topic** - Daily top 7 trending topics
   - `main_article_id`: Representative article
   - `main_stance`: Stance of representative article
   - `topic_rank`: Daily ranking (1-7)
4. **topic_article_mapping** - Many-to-many relationship
   - `similarity_score`: Cosine similarity for clustering
5. **stance_analysis** - Sentiment classification
   - `stance_label`: ÏòπÌò∏/Ï§ëÎ¶Ω/ÎπÑÌåê
   - `stance_score`: Range [-1, 1]
6. **recommended_article** - Top 3 articles per stance per topic

### Key Features
- **pgvector extension**: For embeddings and similarity search
- **Triggers**: Auto-update article counts
- **Indexes**: Optimized for date-range and stance queries

### Database Migrations with Alembic

The project uses **Alembic** for database schema version control and migrations.

**Benefits:**
- Version-controlled schema changes tracked in Git
- Safe schema updates across environments (local/staging/production)
- Rollback capability for failed migrations
- Automatic migration on Render deployment
- Team collaboration without schema conflicts

**Migration Files:**
- `alembic.ini` - Alembic configuration
- `database/migrations/env.py` - Environment setup (reads .env)
- `database/migrations/versions/` - Migration version files
- `scripts/migrate.py` - Helper script for common operations
- `scripts/init_db.py` - Database initialization + seeding

**Common Operations:**
```bash
# Apply all pending migrations
python scripts/migrate.py up

# Create a new migration after schema changes
alembic revision -m "add_column_xyz"

# Show current database version
python scripts/migrate.py current

# View migration history
python scripts/migrate.py history

# Rollback one migration
python scripts/migrate.py down

# Reset database (WARNING: drops all tables)
python scripts/migrate.py reset
```

**Deployment:**
- Migrations run automatically on Render during build
- `buildCommand` in render.yaml: `pip install -r requirements.txt && python scripts/migrate.py up`
- pgvector extension is automatically enabled by initial migration

## Data Pipeline Flow

```
Phase 1: News Collection (Render Cron - Daily 5:00 AM KST)
‚Üí Naver News ‚Üí Selenium Scraper ‚Üí PostgreSQL
‚Üí Enqueue article_ids to Redis

Phase 2: Batch Summarization (Celery Worker ‚Üí AI Service)
‚Üí Redis Queue ‚Üí Celery Worker (batch 50-100)
‚Üí Fetch articles from DB
‚Üí POST /batch-summarize to AI Service
‚Üí Save summaries to DB

Phase 3: Topic Clustering (Render Cron)
‚Üí Generate embeddings from summaries (pgvector)
‚Üí Cosine similarity clustering
‚Üí Select top 7 topics with representative articles
‚Üí Update topic tables
‚Üí Enqueue (article_id, topic_id) pairs

Phase 4: Batch Stance Analysis (Celery Worker ‚Üí AI Service)
‚Üí Redis Queue ‚Üí Celery Worker
‚Üí POST /batch-analyze-stance to AI Service
‚Üí Save stance results to DB

Phase 5: Recommendation & API
‚Üí Recommendation engine selects top 3 per stance
‚Üí FastAPI serves data to frontend
```

## Environment Configuration

### Local Development (.env)
```
DB_HOST=localhost
DB_PORT=5432
DB_NAME=politics_news_dev
DB_USER=postgres
DB_PASSWORD=postgres
REDIS_URL=redis://localhost:6379/0
AI_SERVICE_URL=https://zedwrkc-news-stance-detection.hf.space
AI_SERVICE_TIMEOUT=120
```

### Production (Render Dashboard)
```
DATABASE_URL=<auto-injected by Render>
REDIS_URL=<auto-injected by Render>
AI_SERVICE_URL=https://zedwrkc-news-stance-detection.hf.space
AI_SERVICE_TIMEOUT=120
```

## API Endpoints (To Be Implemented)

```
GET  /health                                    # Health check
GET  /api/v1/topics?date=YYYY-MM-DD            # Daily top 7 topics
GET  /api/v1/topics/{topic_id}/articles        # Articles by topic (grouped by stance)
GET  /api/v1/topics/{topic_id}/recommendations # Top 3 per stance (9 total)
```

## AI Service Integration

The AI service is deployed on Hugging Face Spaces at: `https://zedwrkc-news-stance-detection.hf.space`

### AI Service Details
- **Model**: gogamza/kobart-summarization (KoBART)
- **Framework**: FastAPI + HuggingFace Transformers + PyTorch
- **Device**: CPU (HF Spaces free tier)
- **Batch Size**: Up to 50 articles per request
- **Timeout**: 120 seconds recommended (model inference time)
- **Dynamic Summary Length**: Automatically adjusts based on input article length
  - Short articles (<150 tokens): Adjusted to avoid expansion
  - Medium articles (150-500 tokens): 3-4 sentences
  - Long articles (500+ tokens): 4-5 sentences

### API Endpoints

**GET /health** - Health check
```json
Response: {
  "status": "healthy",
  "summarization_model": "gogamza/kobart-summarization",
  "stance_model": null,
  "device": "cpu"
}
```

**POST /batch-process-articles** (Primary Endpoint - Recommended)
Unified processing endpoint for summarization + optional stance analysis

```json
Request: {
  "articles": [
    {"article_id": 1, "content": "Ï†ïÏπò Îâ¥Ïä§ Î≥∏Î¨∏..."},
    {"article_id": 2, "content": "Îã§Î•∏ Îâ¥Ïä§ Î≥∏Î¨∏..."}
  ],
  "max_summary_length": 300,  // optional, default: 300 tokens
  "min_summary_length": 150   // optional, default: 150 tokens
}
Response: {
  "results": [
    {
      "article_id": 1,
      "summary": "ÏöîÏïΩÎêú ÎÇ¥Ïö© (3-5Î¨∏Ïû•)...",
      "stance": null,  // To be implemented
      "error": null
    },
    {
      "article_id": 2,
      "summary": "Îã§Î•∏ ÏöîÏïΩ ÎÇ¥Ïö©...",
      "stance": null,
      "error": null
    }
  ],
  "total_processed": 2,
  "successful": 2,
  "failed": 0,
  "processing_time_seconds": 2.5
}
```

**POST /batch-summarize** (Legacy - For backward compatibility)
```json
Request: {
  "articles": [
    {"article_id": 1, "content": "..."},
    {"article_id": 2, "content": "..."}
  ],
  "max_length": 128  // optional, default: 128 tokens
}
Response: {
  "summaries": [
    {"article_id": 1, "summary": "...", "error": null},
    {"article_id": 2, "summary": "...", "error": null}
  ],
  "total_processed": 2,
  "successful": 2,
  "failed": 0
}
```

**POST /batch-analyze-stance** (To Be Implemented)
```json
Request: {
  "items": [
    {"article_id": 1, "topic": "...", "summary": "..."},
    {"article_id": 2, "topic": "...", "summary": "..."}
  ]
}
Response: {
  "results": [
    {
      "article_id": 1,
      "stance_label": "ÏòπÌò∏",
      "prob_positive": 0.7,
      "prob_neutral": 0.2,
      "prob_negative": 0.1
    }
  ]
}
```

## Deployment to Render

### Prerequisites
1. Render account
2. GitHub repository connected
3. Redis instance created in Render Dashboard
4. AI service deployed and URL configured

### Deployment Steps

```bash
# 1. Push code to GitHub
git add .
git commit -m "Initial backend setup"
git push origin main

# 2. In Render Dashboard:
#    - New ‚Üí Blueprint
#    - Connect to GitHub repository
#    - Render will read render.yaml
#    - Configure environment variables (AI_SERVICE_URL, CORS_ORIGINS)
#    - Deploy

# 3. Enable pgvector extension:
#    - Go to PostgreSQL instance
#    - Connect tab ‚Üí Run SQL
#    - CREATE EXTENSION IF NOT EXISTS vector;

# 4. Initialize database schema:
#    - Run init_db.py as a one-time job
#    - Or manually run postgre_schema.sql via psql
```

## Common Commands

```bash
# Run tests
pytest tests/

# Code formatting
black src/

# Linting
flake8 src/

# Type checking
mypy src/

# Database migrations
python scripts/migrate.py up              # Run all pending migrations
python scripts/migrate.py down            # Downgrade one revision
python scripts/migrate.py current         # Show current revision
python scripts/migrate.py history         # Show migration history
python scripts/migrate.py reset           # Reset database (WARNING: drops all tables)

# Create new migration (after schema changes)
alembic revision -m "description_of_change"

# Initialize fresh database
python scripts/init_db.py                 # Run migrations and seed data
python scripts/init_db.py --reset         # Drop all tables and reinitialize
```

## Important Notes

### Security
- `.env` file is git-ignored
- Never commit credentials
- Use Render environment variables for production

### Performance
- Batch size: 50-100 articles per API call
- Redis queue prevents blocking operations
- pgvector for fast similarity search
- Denormalized `recommended_article` table for fast reads

### Development Guidelines
1. Always activate virtual environment before working
2. Keep Docker services running during development
3. Test with small datasets first
4. Use comprehensive logging
5. Handle partial failures in batch processing
6. Ensure idempotency in Celery tasks

### Three-Stance System
All articles are classified as:
- **ÏòπÌò∏** (Support/Positive)
- **Ï§ëÎ¶Ω** (Neutral)
- **ÎπÑÌåê** (Criticism/Negative)

### Daily News Cycle
- Uses KST 5:00 AM cutoff for `news_date`
- Scraper runs daily at 5:00 AM KST (Render Cron)
- Clustering runs after summarization completes

## Current Implementation Status

### ‚úÖ Completed

#### Infrastructure & Setup (2025-10-17)
- Project structure and file organization
- Docker Compose setup (PostgreSQL + Redis)
- Database schema with pgvector extension
- Environment configuration (.env setup)
- Render deployment configuration (render.yaml)
- Git repository setup
- Python dependencies installation (requirements.txt)

#### Database Migration System - COMPLETED ‚úÖ (2025-10-18)
**Files Created/Modified:**
- `alembic.ini` - Alembic configuration
- `database/migrations/env.py` - Migration environment setup
- `database/migrations/versions/1fdac3e26595_initial_schema.py` - Initial migration
- `scripts/migrate.py` - Migration helper script
- `scripts/init_db.py` - Enhanced database initialization script
- `requirements.txt` - Added alembic==1.13.2
- `render.yaml` - Added auto-migration to buildCommand

**Features Implemented:**
1. **Alembic Integration**
   - ‚úÖ Alembic initialized with proper configuration
   - ‚úÖ Environment-aware database URL (reads from .env or DATABASE_URL)
   - ‚úÖ Initial migration created from existing schema
   - ‚úÖ Support for both local and production environments

2. **Migration Scripts**
   - ‚úÖ `scripts/migrate.py` - User-friendly migration commands
   - ‚úÖ `scripts/init_db.py` - Complete database setup (migrations + seeding)
   - ‚úÖ Database verification and health checks
   - ‚úÖ Safe reset functionality with confirmation

3. **Automated Deployment**
   - ‚úÖ Migrations run automatically on Render build
   - ‚úÖ pgvector extension automatically enabled
   - ‚úÖ No manual SQL execution needed

4. **Benefits Achieved:**
   - ‚úÖ Version-controlled schema changes
   - ‚úÖ Rollback capability for failed migrations
   - ‚úÖ Team collaboration without conflicts
   - ‚úÖ Consistent schema across all environments

#### Phase 1: News Collection - COMPLETED ‚úÖ
**Files Created/Modified:**
- `src/models/database.py` - Database models and repositories
- `src/utils/logger.py` - Logging configuration
- `src/scrapers/scraper.py` - Refactored scraper with DB integration
- `scripts/run_scraper.py` - Scraper execution script
- `.env` - Local environment configuration

**Features Implemented:**
1. **Database Layer** (`src/models/database.py`)
   - Connection pool management with `psycopg2`
   - `PressRepository`: Press organization CRUD operations
   - `ArticleRepository`: Article CRUD operations with duplicate checking
   - KST timezone handling with UTC conversion
   - `news_date` auto-calculation (5:00 AM KST cutoff)
   - Context managers for safe DB operations

2. **Scraper Refactor** (`src/scrapers/scraper.py`)
   - ‚úÖ Removed JSON file output ‚Üí Direct database save
   - ‚úÖ Class-based architecture (`NaverNewsScraper`)
   - ‚úÖ Duplicate checking by URL
   - ‚úÖ Error handling with try-except blocks
   - ‚úÖ Statistics tracking (scraped, saved, duplicates, errors)
   - ‚úÖ KST timezone support
   - ‚úÖ Infinite scroll handling (max 50 scrolls)
   - ‚úÖ Rate limiting (configurable delay)
   - ‚úÖ Support for 6 press companies:
     - Ïó∞Ìï©Îâ¥Ïä§ (001)
     - Ï°∞ÏÑ†ÏùºÎ≥¥ (023)
     - ÎèôÏïÑÏùºÎ≥¥ (020)
     - YTN (052)
     - ÌïúÍ≤®Î†à (028)
     - Í≤ΩÌñ•Ïã†Î¨∏ (032)

3. **Key Bug Fixes:**
   - ‚úÖ Schema column name matching (url ‚Üí article_url, thumbnail_url ‚Üí img_url)
   - ‚úÖ press_id type correction (integer ‚Üí varchar)
   - ‚úÖ Timezone constraint handling (KST ‚Üí UTC conversion)
   - ‚úÖ Database connection pool management
   - ‚úÖ Environment variable precedence (.env vs system env)

4. **Testing Results:**
   ```
   ‚úì Scraper successfully connects to local PostgreSQL
   ‚úì Articles saved to database with correct schema
   ‚úì Duplicate detection working
   ‚úì Timezone conversion (KST ‚Üí UTC) working
   ‚úì news_date auto-calculation working
   ‚úì Error handling and logging working
   ```

5. **Database Schema Verification:**
   - All 6 tables created and indexed
   - pgvector extension enabled (v0.8.1)
   - Foreign key constraints working
   - Triggers for article count auto-update working

### üöß In Progress
- Celery task implementation
- AI service client
- FastAPI endpoints
- Topic clustering algorithm
- Recommendation engine

### üìã TODO (Remaining Phases)

**Phase 2: Batch Summarization**
- Implement Celery workers
- Create AI service client
- Build batch processing queue

**Phase 3: Topic Clustering**
- Implement clustering algorithm
- Generate embeddings with pgvector
- Select top 7 topics

**Phase 4: Batch Stance Analysis**
- Integrate stance analysis AI model
- Process articles by topic

**Phase 5: Recommendation & API**
- Build recommendation engine
- Create FastAPI endpoints
- Deploy to Render

**General:**
- Write tests (pytest)
- Deploy to Render
- Integrate with frontend
- Performance optimization
- Monitoring and logging enhancements

## Next Steps

1. ~~**Improve Scraper**~~ ‚úÖ COMPLETED
   - ~~Remove JSON file output, save directly to DB~~
   - ~~Add error handling and retry logic~~
   - ~~Implement duplicate checking~~

2. **Implement Celery Tasks** (`src/workers/tasks.py`)
   - `process_summarization_batch(article_ids)`
   - `process_stance_analysis_batch(article_topic_pairs)`
   - Add retry logic with exponential backoff

3. **Build AI Service Client** (`src/services/ai_client.py`)
   - HTTP client for batch API calls
   - Timeout and error handling
   - Partial failure tolerance

4. **Implement Clustering** (`src/services/clustering.py`)
   - Generate embeddings with pgvector
   - Cosine similarity based clustering
   - Select top 7 topics

5. **Create FastAPI Endpoints** (`src/api/routes/`)
   - Topics endpoint
   - Articles endpoint
   - Recommendations endpoint
   - Health check

6. **Deploy and Test**
   - Push to Render
   - Verify Cron jobs
   - Test full pipeline end-to-end

## Troubleshooting

### Docker Issues
```bash
# Reset everything
docker-compose down -v
docker system prune -a
docker-compose up -d
```

### Database Connection Issues
```bash
# Check PostgreSQL is running
docker-compose ps
# Check logs
docker-compose logs postgres
# Test connection
psql postgresql://postgres:postgres@localhost:5432/politics_news_dev
```

### Redis Issues
```bash
# Check Redis is running
docker-compose ps
# Test connection
redis-cli -h localhost -p 6379 ping
```

## Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Celery Documentation](https://docs.celeryq.dev/)
- [Render Documentation](https://render.com/docs)
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Hugging Face Spaces](https://huggingface.co/spaces)
