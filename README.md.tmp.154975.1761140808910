# Political News Backend

Backend service for Korean political news aggregation and analysis system.

## ğŸš€ Features

- **News Scraping**: Automated scraping from 6 major Korean news sources (30ë¶„ ì£¼ê¸°)
- **AI Processing**: Summarization + 768-dim embedding generation via HF Spaces
- **Topic Clustering**: Hierarchical Clustering (5-10 topics, 2ì‹œê°„ ì£¼ê¸°) â­
- **Incremental Assignment**: Real-time article-to-topic matching using centroid similarity (30ë¶„ ì£¼ê¸°)
- **30ë¶„ íŒŒì´í”„ë¼ì¸**: Scraping â†’ AI Processing â†’ Incremental Assignment (Celery Chain) â­
- **Database**: PostgreSQL with pgvector extension for similarity search
- **Task Queue**: Celery + Redis for async processing
- **Migrations**: Alembic for version-controlled schema management

## ğŸ“‹ Tech Stack

- **Python 3.12** with virtual environment
- **FastAPI 0.119.0** - REST API framework (TODO)
- **PostgreSQL 16** + **pgvector** - Vector database
- **Redis** - Task queue & caching
- **Celery** - Async task processing
- **Selenium 4.35.0** - Web scraping
- **scikit-learn** - ML clustering
- **Alembic 1.13.2** - Database migrations

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scraper   â”‚â”€â”€â”€â”€â–¶â”‚  PostgreSQL  â”‚â—€â”€â”€â”€â”€â”‚   Celery    â”‚
â”‚  (Naver)    â”‚     â”‚  + pgvector  â”‚     â”‚   Worker    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                    â”‚
                            â”‚                    â–¼
                            â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚            â”‚ AI Service   â”‚
                            â”‚            â”‚ (HF Spaces)  â”‚
                            â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   FastAPI    â”‚
                    â”‚ (TODO)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Data Pipeline

### 30ë¶„ ì£¼ê¸°: ìŠ¤í¬ë˜í•‘ + íŒŒì´í”„ë¼ì¸ â­
1. **Scraping** â†’ Collect news from 6 sources
2. **Celery Chain Trigger**:
   - **AI Processing** (ë°°ì¹˜ 5ê°œ) â†’ Summary + Embedding generation
   - **Incremental Assignment** â†’ Assign new articles to topics using centroid similarity

### 2ì‹œê°„ ì£¼ê¸°: Full Re-Clustering â­
3. **Hierarchical Clustering** â†’ Create 5-10 topics (auto-range), save top 7 with centroids

### TODO
4. **API Serving** â†’ Provide data to frontend

## ğŸ› ï¸ Setup

### Prerequisites

- Python 3.12+
- Docker & Docker Compose
- PostgreSQL 16
- Redis

### Installation

```bash
# 1. Navigate to backend directory
cd backend

# 2. Create virtual environment
python3.12 -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env with your settings

# 5. Start Docker services
docker compose up -d

# 6. Initialize database
python scripts/init_db.py

# 7. Verify setup
docker compose ps
python scripts/migrate.py current
```

## ğŸ§ª Running Locally

```bash
# Terminal 1: Start Celery worker
celery -A src.workers.celery_app worker --loglevel=info

# Terminal 2: Run 30ë¶„ pipeline (scraping + AI + incremental) â­
python scripts/run_scraper_with_pipeline.py

# OR run components manually:

# Terminal 2a: Run scraper only
python scripts/run_scraper.py

# Terminal 3: Run clustering (Hierarchical, 2ì‹œê°„ ì£¼ê¸° ì‹œë®¬ë ˆì´ì…˜) â­
python scripts/run_clustering.py 2025-10-20 hierarchical

# Terminal 4: Run incremental assignment
python scripts/incremental_assign.py --date 2025-10-20

# Terminal 5: Start FastAPI (TODO)
# uvicorn src.api.main:app --reload --port 8000
```

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                    # FastAPI application (TODO)
â”‚   â”œâ”€â”€ scrapers/               # Naver News scraper âœ…
â”‚   â”œâ”€â”€ workers/                # Celery tasks âœ…
â”‚   â”œâ”€â”€ services/               # Business logic âœ…
â”‚   â”‚   â”œâ”€â”€ clustering.py       # Topic clustering
â”‚   â”‚   â”œâ”€â”€ incremental_assignment.py  # Incremental assignment
â”‚   â”‚   â””â”€â”€ ai_client.py        # AI service client
â”‚   â”œâ”€â”€ models/                 # Database layer âœ…
â”‚   â”œâ”€â”€ utils/                  # Utilities âœ…
â”‚   â””â”€â”€ config.py               # Configuration âœ…
â”‚
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ migrations/             # Alembic migrations âœ…
â”‚   â””â”€â”€ postgre_schema.sql      # Schema reference
â”‚
â”œâ”€â”€ scripts/                    # Executable scripts âœ…
â”‚   â”œâ”€â”€ run_scraper_with_pipeline.py  # 30ë¶„ pipeline â­
â”‚   â”œâ”€â”€ run_scraper.py          # Scraper only
â”‚   â”œâ”€â”€ run_clustering.py       # Clustering (hierarchical) â­
â”‚   â”œâ”€â”€ incremental_assign.py   # Incremental assignment
â”‚   â”œâ”€â”€ init_db.py              # Database initialization
â”‚   â””â”€â”€ migrate.py              # Migration helper
â”‚
â”œâ”€â”€ test_ai_pipeline.py         # End-to-end test âœ…
â”œâ”€â”€ docker-compose.yml          # Local development
â”œâ”€â”€ render.yaml                 # Render deployment
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ alembic.ini                 # Alembic config
```

## ğŸ—„ï¸ Database Schema

### Core Tables

1. **press** - News organizations (6 sources)
2. **article** - Full content + summary + embedding (768-dim)
3. **topic** - Daily top 7 topics with centroids
4. **topic_article_mapping** - Article-to-topic assignments
5. **stance_analysis** - Sentiment classification (TODO)
6. **recommended_article** - Top 3 per stance (TODO)
7. **pending_articles** - Unmatched articles

### Key Features

- **pgvector extension**: Vector similarity search
- **Alembic migrations**: Version-controlled schema
- **IVFFlat index**: Fast centroid matching
- **Triggers**: Auto-update article counts

## ğŸ”§ Database Migrations

```bash
# Apply all pending migrations
python scripts/migrate.py up

# Create new migration
alembic revision -m "description"

# Check current version
python scripts/migrate.py current

# View history
python scripts/migrate.py history

# Rollback one version
python scripts/migrate.py down

# Reset database (âš ï¸ destroys all data)
python scripts/migrate.py reset
```

## ğŸ“ Common Commands

```bash
# Run 30ë¶„ pipeline (recommended) â­
python scripts/run_scraper_with_pipeline.py

# Run scraper only
python scripts/run_scraper.py

# Run clustering (hierarchical) â­
python scripts/run_clustering.py [date] hierarchical

# Run incremental assignment
python scripts/incremental_assign.py [--date YYYY-MM-DD] [--dry-run]

# Start Celery worker
celery -A src.workers.celery_app worker --loglevel=info

# Run tests
pytest tests/

# Code formatting
black src/

# Linting
flake8 src/
```

## ğŸŒ Environment Variables

### Required

- `DB_HOST` - PostgreSQL host (default: localhost)
- `DB_PORT` - PostgreSQL port (default: 5432)
- `DB_NAME` - Database name
- `DB_USER` - Database user
- `DB_PASSWORD` - Database password
- `REDIS_URL` - Redis connection URL
- `AI_SERVICE_URL` - AI service endpoint

### Optional (Clustering)

- `CLUSTERING_ALGORITHM` - Algorithm (default: hierarchical) â­
- `CLUSTERING_DISTANCE_THRESHOLD` - Distance threshold (default: 0.5) â­
- `CLUSTERING_MIN_TOPICS` - Min topics (default: 5) â­
- `CLUSTERING_MAX_TOPICS` - Max topics (default: 10) â­
- `CLUSTERING_TOP_N` - Top N to save (default: 7)

### Optional (Incremental Assignment)

- `INCREMENTAL_SIMILARITY_THRESHOLD` - Similarity threshold (default: 0.5)
- `INCREMENTAL_CENTROID_UPDATE_WEIGHT` - Centroid update weight (default: 0.1)

## ğŸš¢ Deployment (Render)

### Prerequisites

1. Render account
2. PostgreSQL instance (with pgvector)
3. Redis instance
4. AI service deployed (HF Spaces)

### Steps

```bash
# 1. Push to GitHub
git add .
git commit -m "Deploy backend"
git push origin main

# 2. In Render Dashboard:
#    - New â†’ Blueprint
#    - Connect GitHub repository
#    - Render reads backend/render.yaml
#    - Configure environment variables
#    - Deploy

# 3. Migrations run automatically
#    (via buildCommand in render.yaml)
```

### Render Services

- **Web Service**: FastAPI backend (TODO)
- **Background Worker**: Celery worker
- **Cron Jobs**:
  - **30ë¶„ ì£¼ê¸°**: Scraper + Pipeline (scraping â†’ AI â†’ incremental) â­
  - **2ì‹œê°„ ì£¼ê¸°**: Full Re-Clustering (hierarchical, 5-10 topics) â­

## ğŸ“Š Current Status (2025-10-22)

### ğŸ“ˆ Statistics

- **Articles Collected**: 243 total (5 press sources)
  - YTN (052): 99 articles
  - SBS (020): 47 articles
  - ê²½í–¥ì‹ ë¬¸ (032): 47 articles
  - í•œê²¨ë ˆ (028): 27 articles
  - ì¡°ì„ ì¼ë³´ (023): 23 articles
  - ~~ì—°í•©ë‰´ìŠ¤ (001)~~: Excluded for testing

- **AI Processing**: 227/243 articles (93% success rate)
  - Batch size: 5 articles
  - Failed: 16 articles (AI model "index out of range" errors)
  - Processing time: ~30-50s per batch

- **Topics Created**: 7 topics for 2025-10-20
  1. êµ­ì •ê°ì‚¬ 'ê¹€í˜„ì§€ ê³µë°©' (104 articles)
  2. ì¬íŒì†Œì› ë‹¹ë¡  ì¶”ì§„ (66 articles)
  3. ì£¼íƒì‹œì¥ ì•ˆì • (56 articles)
  4. ìº„ë³´ë””ì•„ ê°ê¸ˆ ì‚¬ê±´ (56 articles)
  5. ìœ¤ ëŒ€í†µë ¹ ë©´íšŒ ë…¼ë€ (52 articles)
  6. ë°©ì‚°Â·í•­ê³µìš°ì£¼ íˆ¬ì (46 articles)
  7. ë‚¨ë¶í•œ í†µì¼ ì—¬ë¡ ì¡°ì‚¬ (20 articles)

### âœ… Completed Phases

- âœ… Phase 1: News Collection (Scraper + DB integration)
- âœ… Phase 2: Backend-AI Integration (Celery + AI client with HF Spaces warmup)
- âœ… Phase 3: Topic Clustering (Hierarchical + centroid storage) â­
- âœ… Phase 3.5: Incremental Assignment (Centroid-based matching)
- âœ… 30ë¶„ Pipeline (Scraping â†’ AI â†’ Incremental) â­
- âœ… 2ì‹œê°„ Re-Clustering (Hierarchical, auto-range 5-10) â­
- âœ… Database Migrations (Alembic)

### ğŸš§ In Progress

- None

### ğŸ“‹ TODO (Next Priority)

- â­ï¸ Phase 4: Stance Analysis (waiting for ML model)
- â­ï¸ **Phase 5: FastAPI Endpoints (HIGH PRIORITY)**
  - Health check
  - Topics API
  - Articles API
  - Recommendations API
- â­ï¸ Recommendation Engine (top 3 per stance)
- â­ï¸ Frontend Integration

## ğŸ§ª Testing

### Unit Tests

```bash
pytest tests/
```

### Integration Tests

```bash
# End-to-end pipeline test
python test_ai_pipeline.py
```

### Manual Testing

```bash
# Test scraper
python scripts/run_scraper.py

# Test 30ë¶„ pipeline â­
python scripts/run_scraper_with_pipeline.py

# Test clustering (hierarchical) â­
python scripts/run_clustering.py 2025-10-20 hierarchical

# Test incremental assignment (dry-run)
python scripts/incremental_assign.py --date 2025-10-20 --dry-run
```

## ğŸ› Troubleshooting

### Docker Issues

```bash
# Reset Docker
docker compose down -v
docker compose up -d
```

### Database Connection

```bash
# Check PostgreSQL
docker compose ps
docker compose logs postgres

# Test connection
psql postgresql://postgres:postgres@localhost:5432/politics_news_dev
```

### Redis Connection

```bash
# Check Redis
docker compose ps
redis-cli -h localhost -p 6379 ping
```

### Migration Issues

```bash
# Check migration status
python scripts/migrate.py current

# Reset database (âš ï¸ destroys data)
python scripts/migrate.py reset
python scripts/init_db.py
```

## ğŸ“š Resources

- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Celery Docs](https://docs.celeryq.dev/)
- [pgvector Docs](https://github.com/pgvector/pgvector)
- [Alembic Docs](https://alembic.sqlalchemy.org/)
- [Render Docs](https://render.com/docs)

## ğŸ“„ License

MIT

## ğŸ‘¥ Team

- Backend Developer: Scraper, API, Celery, deployment
- Frontend Developer: React application (separate repo)
- ML Engineer: Stance analysis model (Colab)

---

**AI Service**: https://zedwrkc-news-stance-detection.hf.space
