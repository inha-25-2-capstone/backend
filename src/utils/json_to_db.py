"""
JSON íŒŒì¼ì˜ ë‚´ìš©ì„ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. Bulk Insertë¥¼ ì‚¬ìš©í•œ ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ (100ë°° ì´ìƒ ì„±ëŠ¥ í–¥ìƒ)
2. ì²­í¬ ë‹¨ìœ„ ì»¤ë°‹ìœ¼ë¡œ ë¶€ë¶„ ì‹¤íŒ¨ ì‹œì—ë„ ì„±ê³µí•œ ë°ì´í„° ë³´ì¡´
3. SAVEPOINTë¥¼ í™œìš©í•œ ì•ˆì •ì ì¸ íŠ¸ëœì­ì…˜ ê´€ë¦¬
4. ìƒì„¸í•œ ì§„í–‰ ìƒí™© ë° ì—ëŸ¬ ë¡œê¹…
"""
import json
import psycopg2
from psycopg2.extras import execute_values
from datetime import datetime, timedelta
from config import DATABASE_URL
import sys
import os


# ì„¤ì • ìƒìˆ˜
CHUNK_SIZE = 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜


def calculate_news_date(published_at_str):
    """
    ê¸°ì‚¬ ë°œí–‰ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ 'news_date'ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    KST 05:00ì‹œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ë£¨ê°€ ë°”ë€ë‹ˆë‹¤.

    ì˜ˆì‹œ:
    - 2025-10-17 04:59:59 -> 2025-10-16
    - 2025-10-17 05:00:00 -> 2025-10-17

    Args:
        published_at_str: "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ ë¬¸ìì—´

    Returns:
        news_date: "YYYY-MM-DD" í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´
    """
    try:
        published_at = datetime.strptime(published_at_str, "%Y-%m-%d %H:%M:%S")
        if published_at.hour < 5:
            news_date = (published_at - timedelta(days=1)).date()
        else:
            news_date = published_at.date()
        return str(news_date)
    except Exception as e:
        print(f"  âš ï¸  ë‚ ì§œ ë³€í™˜ ì˜¤ë¥˜: {published_at_str}, ì˜¤ë¥˜: {e}")
        return None


def get_press_id_from_url(url):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ ì–¸ë¡ ì‚¬ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ì˜ˆì‹œ: https://n.news.naver.com/article/001/0015683856 -> "001"

    Args:
        url: ë„¤ì´ë²„ ë‰´ìŠ¤ ì›ë¬¸ URL

    Returns:
        press_id: ì–¸ë¡ ì‚¬ ID (3ìë¦¬ ë¬¸ìì—´)
    """
    try:
        parts = url.split('/')
        if 'article' in parts and len(parts) > parts.index('article') + 1:
            return parts[parts.index('article') + 1]
        return None
    except Exception as e:
        print(f"  âš ï¸  URLì—ì„œ ì–¸ë¡ ì‚¬ ID ì¶”ì¶œ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
        return None


def load_json_file(filename):
    """
    JSON íŒŒì¼ì„ ì½ì–´ íŒŒì´ì¬ ê°ì²´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        filename: JSON íŒŒì¼ ì´ë¦„

    Returns:
        articles: ê¸°ì‚¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    try:
        if not os.path.exists(filename):
            print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {filename}")
            return None

        with open(filename, 'r', encoding='utf-8') as f:
            articles = json.load(f)

        print(f"âœ… JSON íŒŒì¼ ë¡œë”© ì™„ë£Œ: ì´ {len(articles)}ê°œì˜ ê¸°ì‚¬")
        return articles
    except Exception as e:
        print(f"âŒ JSON íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return None


def bulk_insert_press(cursor, press_data_list):
    """
    ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ì¼ê´„ ì‚½ì…í•©ë‹ˆë‹¤ (Bulk Insert).
    ì¤‘ë³µëœ ê²½ìš° ë¬´ì‹œí•©ë‹ˆë‹¤.

    Args:
        cursor: psycopg2 ì»¤ì„œ ê°ì²´
        press_data_list: [(press_id, press_name), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸

    Returns:
        inserted_count: ì‹¤ì œ ì‚½ì…ëœ ì–¸ë¡ ì‚¬ ìˆ˜
    """
    if not press_data_list:
        return 0

    try:
        # ì¤‘ë³µ ì œê±° (press_id ê¸°ì¤€)
        unique_press = {}
        for press_id, press_name in press_data_list:
            if press_id not in unique_press:
                unique_press[press_id] = press_name

        press_values = list(unique_press.items())

        query = """
            INSERT INTO press (press_id, press_name)
            VALUES %s
            ON CONFLICT (press_id) DO NOTHING
        """

        execute_values(cursor, query, press_values, template="(%s, %s)")
        return cursor.rowcount
    except Exception as e:
        print(f"  âš ï¸  ì–¸ë¡ ì‚¬ ì¼ê´„ ì‚½ì… ì˜¤ë¥˜: {e}")
        return 0


def bulk_insert_articles(cursor, article_data_list):
    """
    ê¸°ì‚¬ ì •ë³´ë¥¼ ì¼ê´„ ì‚½ì…í•©ë‹ˆë‹¤ (Bulk Insert).
    ì¤‘ë³µëœ URLì¸ ê²½ìš° ë¬´ì‹œí•©ë‹ˆë‹¤.

    Args:
        cursor: psycopg2 ì»¤ì„œ ê°ì²´
        article_data_list: [(press_id, news_date, author, title, content,
                             article_url, img_url, published_at), ...] ë¦¬ìŠ¤íŠ¸

    Returns:
        inserted_count: ì‹¤ì œ ì‚½ì…ëœ ê¸°ì‚¬ ìˆ˜
    """
    if not article_data_list:
        return 0

    try:
        query = """
            INSERT INTO article (
                press_id, news_date, author, title, content,
                article_url, img_url, published_at
            )
            VALUES %s
            ON CONFLICT (article_url) DO NOTHING
        """

        execute_values(
            cursor,
            query,
            article_data_list,
            template="(%s, %s, %s, %s, %s, %s, %s, %s)"
        )
        return cursor.rowcount
    except Exception as e:
        print(f"  âš ï¸  ê¸°ì‚¬ ì¼ê´„ ì‚½ì… ì˜¤ë¥˜: {e}")
        raise  # ìƒìœ„ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ì˜ˆì™¸ ì „íŒŒ


def prepare_article_data(article, press_id):
    """
    JSON ê¸°ì‚¬ ë°ì´í„°ë¥¼ DB ì‚½ì…ìš© íŠœí”Œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        article: JSON ê¸°ì‚¬ ë°ì´í„° (dict)
        press_id: ì–¸ë¡ ì‚¬ ID

    Returns:
        tuple: DB ì‚½ì…ìš© ë°ì´í„° ë˜ëŠ” None (ì˜¤ë¥˜ ì‹œ)
    """
    try:
        news_date = calculate_news_date(article['date'])
        if not news_date:
            return None

        # published_atì„ KST íƒ€ì„ì¡´ìœ¼ë¡œ ëª…ì‹œ
        # JSONì˜ 'date'ëŠ” KST ê¸°ì¤€ "YYYY-MM-DD HH:MM:SS" í˜•ì‹
        published_at_kst = article['date'] + '+09:00'  # KST = UTC+9

        return (
            press_id,
            news_date,
            article.get('author'),      # NULL í—ˆìš©
            article['title'],
            article['content'],
            article['url'],
            article.get('img'),         # NULL í—ˆìš©
            published_at_kst            # íƒ€ì„ì¡´ ì •ë³´ í¬í•¨
        )
    except KeyError as e:
        print(f"  âš ï¸  í•„ìˆ˜ í‚¤ ëˆ„ë½: {e}, URL: {article.get('url', 'N/A')}")
        return None
    except Exception as e:
        print(f"  âš ï¸  ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {e}, URL: {article.get('url', 'N/A')}")
        return None


def process_chunk(cursor, articles_chunk, chunk_idx, total_chunks):
    """
    ì²­í¬ ë‹¨ìœ„ë¡œ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    SAVEPOINTë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶€ë¶„ ì‹¤íŒ¨ ì‹œì—ë„ ë‹¤ë¥¸ ì²­í¬ëŠ” ë³´ì¡´í•©ë‹ˆë‹¤.

    Args:
        cursor: psycopg2 ì»¤ì„œ ê°ì²´
        articles_chunk: ì²˜ë¦¬í•  ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        chunk_idx: í˜„ì¬ ì²­í¬ ë²ˆí˜¸
        total_chunks: ì „ì²´ ì²­í¬ ìˆ˜

    Returns:
        dict: ì²˜ë¦¬ ê²°ê³¼ í†µê³„
    """
    stats = {
        'processed': 0,
        'press_inserted': 0,
        'articles_inserted': 0,
        'skipped': 0,
        'errors': 0
    }

    savepoint_name = f"chunk_{chunk_idx}"

    try:
        # SAVEPOINT ìƒì„±
        cursor.execute(f"SAVEPOINT {savepoint_name}")

        # 1ë‹¨ê³„: ì–¸ë¡ ì‚¬ ë°ì´í„° ì¤€ë¹„
        press_data = []
        article_data = []

        for article in articles_chunk:
            stats['processed'] += 1

            # ì–¸ë¡ ì‚¬ ID ì¶”ì¶œ
            press_id = article.get('press_id') or get_press_id_from_url(article['url'])
            if not press_id:
                print(f"  âš ï¸  ì–¸ë¡ ì‚¬ ID ì¶”ì¶œ ì‹¤íŒ¨: {article.get('url', 'N/A')}")
                stats['skipped'] += 1
                continue

            # ì–¸ë¡ ì‚¬ ë°ì´í„° ìˆ˜ì§‘
            press_data.append((press_id, article['press']))

            # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
            article_tuple = prepare_article_data(article, press_id)
            if article_tuple:
                article_data.append(article_tuple)
            else:
                stats['skipped'] += 1

        # 2ë‹¨ê³„: ì¼ê´„ ì‚½ì…
        stats['press_inserted'] = bulk_insert_press(cursor, press_data)
        stats['articles_inserted'] = bulk_insert_articles(cursor, article_data)

        # SAVEPOINT í•´ì œ (ì„±ê³µ)
        cursor.execute(f"RELEASE SAVEPOINT {savepoint_name}")

        print(f"  âœ… ì²­í¬ {chunk_idx}/{total_chunks} ì™„ë£Œ: "
              f"ì–¸ë¡ ì‚¬ {stats['press_inserted']}ê°œ, "
              f"ê¸°ì‚¬ {stats['articles_inserted']}ê°œ ì‚½ì…")

    except Exception as e:
        # SAVEPOINTë¡œ ë¡¤ë°± (ì´ ì²­í¬ë§Œ ì·¨ì†Œ)
        cursor.execute(f"ROLLBACK TO SAVEPOINT {savepoint_name}")
        stats['errors'] = len(articles_chunk)
        print(f"  âŒ ì²­í¬ {chunk_idx}/{total_chunks} ì‹¤íŒ¨: {e}")

    return stats


def main():
    """
    ë©”ì¸ í•¨ìˆ˜: JSON íŒŒì¼ì„ ì½ì–´ ë°ì´í„°ë² ì´ìŠ¤ì— ì¼ê´„ ì‚½ì…í•©ë‹ˆë‹¤.
    """
    # 1. íŒŒì¼ëª… ê²°ì •
    if len(sys.argv) > 1:
        json_filename = sys.argv[1]
    else:
        today = datetime.now().strftime("%Y-%m-%d")
        json_filename = f"politics_news_{today}.json"

    print(f"\n{'='*60}")
    print(f"ğŸ“‚ ì²˜ë¦¬í•  JSON íŒŒì¼: {json_filename}")
    print(f"{'='*60}\n")

    # 2. JSON íŒŒì¼ ë¡œë“œ
    articles = load_json_file(json_filename)
    if not articles:
        return

    # 3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = None
    try:
        print(f"ğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„...")
        conn = psycopg2.connect(DATABASE_URL)
        cursor = conn.cursor()
        print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ\n")
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # 4. ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
    try:
        total_articles = len(articles)
        total_chunks = (total_articles + CHUNK_SIZE - 1) // CHUNK_SIZE

        print(f"ğŸ“Š ì²˜ë¦¬ ê³„íš:")
        print(f"  - ì´ ê¸°ì‚¬ ìˆ˜: {total_articles}ê°œ")
        print(f"  - ì²­í¬ í¬ê¸°: {CHUNK_SIZE}ê°œ")
        print(f"  - ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ\n")
        print(f"{'='*60}")
        print(f"ğŸš€ ë°ì´í„° ì‚½ì… ì‹œì‘...\n")

        # ì „ì²´ í†µê³„
        total_stats = {
            'processed': 0,
            'press_inserted': 0,
            'articles_inserted': 0,
            'skipped': 0,
            'errors': 0
        }

        # ì²­í¬ë³„ ì²˜ë¦¬
        for i in range(0, total_articles, CHUNK_SIZE):
            chunk = articles[i:i + CHUNK_SIZE]
            chunk_idx = i // CHUNK_SIZE + 1

            stats = process_chunk(cursor, chunk, chunk_idx, total_chunks)

            # í†µê³„ ëˆ„ì 
            for key in total_stats:
                total_stats[key] += stats[key]

        # 5. ì „ì²´ ì»¤ë°‹
        conn.commit()
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ ëª¨ë“  ë³€ê²½ì‚¬í•­ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ì»¤ë°‹ë˜ì—ˆìŠµë‹ˆë‹¤")

        # 6. ìµœì¢… í†µê³„ ì¡°íšŒ
        cursor.execute("SELECT COUNT(*) FROM press")
        total_press = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM article")
        total_articles_in_db = cursor.fetchone()[0]

        # 7. ê²°ê³¼ ì¶œë ¥
        print(f"{'='*60}")
        print(f"ğŸ‰ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"\nğŸ“ˆ ì²˜ë¦¬ í†µê³„:")
        print(f"  - ì´ ì²˜ë¦¬ ëŒ€ìƒ: {total_stats['processed']}ê°œ")
        print(f"  - âœ… ì‹ ê·œ ì‚½ì…ëœ ì–¸ë¡ ì‚¬: {total_stats['press_inserted']}ê°œ")
        print(f"  - âœ… ì‹ ê·œ ì‚½ì…ëœ ê¸°ì‚¬: {total_stats['articles_inserted']}ê°œ")
        print(f"  - âš ï¸  ê±´ë„ˆë›´ ê¸°ì‚¬: {total_stats['skipped']}ê°œ")
        print(f"  - âŒ ì˜¤ë¥˜ ë°œìƒ: {total_stats['errors']}ê°œ")
        print(f"\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©:")
        print(f"  - ì´ ì–¸ë¡ ì‚¬ ìˆ˜: {total_press}ê°œ")
        print(f"  - ì´ ê¸°ì‚¬ ìˆ˜: {total_articles_in_db}ê°œ")
        print(f"{'='*60}\n")

    except Exception as e:
        print(f"\nâŒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if conn:
            conn.rollback()
            print(f"ğŸ”„ ëª¨ë“  ë³€ê²½ì‚¬í•­ì´ ë¡¤ë°±ë˜ì—ˆìŠµë‹ˆë‹¤")
    finally:
        if conn:
            cursor.close()
            conn.close()
            print(f"ğŸ”’ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ\n")


if __name__ == "__main__":
    main()
